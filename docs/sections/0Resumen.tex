\begin{abstract}
	Este trabajo presenta una aproximación basada en Aprendizaje por Refuerzo (RL) para resolver la cinemática inversa de los brazos de un robot Pepper. Se diseñaron dos entornos de entrenamiento empleando el framework Gymnasium para modelar los agentes correspondientes a cada brazo y un esquema de Aprendizaje Curricular que incrementa progresivamente la dificultad de los objetivos durante el entrenamiento. Posteriormente, se realizó una optimización de los hiperparámetros de dos algoritmos (PPO y SAC) para identificar cuál se desempeña mejor a la hora de generar una política adecuada para que los brazos alcancen de forma autónoma una posición específica en el espacio. Finalmente, se realizaron unas pruebas de validación con las mejores políticas obtenidas para verificar la precisión alcanzada por los brazos al llegar a poses aleatorias.
\end{abstract}

\vspace{1em}
\noindent\textbf{Palabras clave:} Aprendizaje Curricular, Aprendizaje por Refuerzo, Cinemática Inversa, Pepper

\vspace{6em}
\begin{otherlanguage}{english}
	\begin{abstract}
	This project presents an approach based on Reinforcement Learning to solve the inverse kinematics of a Pepper robot's arms. Two training environments were designed using the Gymnasium framework to model the agents for each arm, together with a Curriculum Learning scheduler that progressively increases task difficulty during training. Next, hyperparameter optimization was carried out for two algorithms (PPO and SAC) to determine which performs better at producing a policy that allows the arms to autonomously reach a given position in space. Finally, validation tests were run with the best policies obtained to verify the accuracy achieved by the arms when reaching random poses.
	\end{abstract}
	
	\vspace{1em}
	\noindent\textbf{Keywords:} Curriculum Learning, Reinforcement Learning, Inverse Kinematics, Pepper
\end{otherlanguage}