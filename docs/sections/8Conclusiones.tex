\section{Conclusiones}

\subsection{Discusión}

A lo largo de este proyecto se diseñaron y compararon dos entornos de entrenamiento para el brazo del robot Pepper, uno analítico y otro simulado, integrando además un esquema de aprendizaje por currículo y un proceso de optimización de hiperparámetros mediante Optuna. El objetivo principal era construir políticas de control capaces de alcanzar con precisión y robustez una posición objetivo en un espacio de trabajo de cinco grados de libertad, incrementando progresivamente la dificultad y ajustando parámetros clave. Las gráficas de Radio de Currículo confirman que el mecanismo de ampliación escalonada permitió exponer al agente a desafíos crecientes, mientras que los incrementos en la Recompensa Media en entrenamiento y en la Tasa de Éxito Promedio evidencian mejoras consistentes en la convergencia de las políticas.\\

Así mismo, el análisis de los estudios de HPO reveló diferencias notables entre los algoritmos PPO y SAC. Las métricas graficadas así como los resultados de comparación de desempeño bajo el protocolo de prueba propuesto permiten concluir preliminarmente que SAC genera, bajo el planteamiento realizado desde el enfoque de un Proceso de Decisión de Markov, un mejor entrenamiento de políticas enfocadas en resolver el problema de la cinemática inversa. De igual manera, las combinaciones de hiperparámetros seleccionadas conducen a comportamientos estables y reproducibles, satisfaciendo el requisito de un agente capaz de adaptarse a variaciones en la posición inicial y a posibles perturbaciones del entorno, factores clave a la hora de extender lo realizado a un robot físico.\\

En síntesis, por medio del entrenamiento de agentes que modelan los brazos izquierdo y derecho del robot Pepper, se ha logrado desarrollar una aproximación basada en RL enfocada en el aprendizaje de la cinemática inversa de estos. De esta forma, se consiguió generar políticas de control de cinco grados de libertad capaces de maximizar la recompensa acumulada y mantener una alta tasa de éxito, cumpliendo con el reto de reducir la intervención manual en la configuración de parámetros. La metodología implementada demuestra que la combinación de aprendizaje por refuerzo, aprendizaje curricular y optimización bayesiana es una solución robusta y escalable para el control preciso de manipuladores robóticos.


\subsection{Trabajo Futuro}

Como trabajo futuro se plantea inicialmente la profundización en la sintonización de hiperparámetros, extendiendo los estudios de Optuna a más \textit{trials} y aumentando el horizonte de entrenamiento (por ejemplo, superando los 1.000.000 de timesteps empleados en este proyecto). De esta manera, se puede plantear la exploración de configuraciones que pueden requerir mayor exposición a episodios largos. Esto podría revelar políticas aún más robustas y generalizables, especialmente si se combinan técnicas de búsqueda bayesiana con estrategias como la optimización multiobjetivo.\\

Otro punto esencial será la extensión a la práctica de lo planteado en este proyecto por medio de la integración con un sistema ROS real. Así pues, la siguiente línea de investigación podría consistir en desarrollar un nodo capaz de cargar los modelos exportados por Stable Baselines y ejecutar la política entrenada en un robot físico Pepper. Esto implicará crear wrappers para convertir las observaciones y acciones entre Gymnasium y ROS, así como diseñar pruebas de validación experimental que midan la transferencia sim2real y garanticen la seguridad y precisión en un entorno físico.
